---
title: "German Credit Model Training Notebook"
output: html_document
---

# German Credit model training
This notebook is a guide to creating a Logistic Regression model on the German Credit dataset. The output of this notebook is a trained model artifact.

## Library imports
```{r eval=FALSE}
library(tidymodels)
library(readr)
library(yardstick)
library(jsonlite)
library(purrr)
library(stringr)
library(dplyr)
```

## Preparing the data
Let's import the data. This specific version of the German Credit dataset has a `label` column that specifies whether or not someone defaulted or paid off the loan. 

```{r}
df = readr::read_csv("./data/german_credit_data.csv", show_col_types = FALSE)
```

For classification models in R, we need the outcome variable to be a factor type. Currently, `label` is binary (0/1).

```{r}
# converting binary label to strings and then to factor
df <- transform(df, label = ifelse((label>0), "Default", "Pay Off"))
df <- df %>% mutate(label = factor(label))
```

Let's also set a random seed for reproducibility
```{r}
set.seed(42)
```

## Preparing for preprocessing
At this point, we'll be splitting the dataset into train/test datasets.

```{r}
# train/test split
df_split <- initial_split(df, prop=0.8, strata=label)
df_baseline <- training(df_split)
df_sample <- testing(df_split)

glimpse(df_baseline)
```
Saving the datasets:
```{r}
# save the train and test data for later use
write_csv(df_baseline, './data/df_baseline.csv')
write_csv(df_sample, './data/df_sample.csv')
```

## Preprocessing and Feature Engineering
Now that the datasets are saved, let's get into preprocessing the data. We'll be using a recipe to conform the data in a workflow. Here are the steps of the recipe:

- First, we drop the `id`, `gender`, and `age_years` columns, as incorporating the last two into the model would introduce explicit bias. We will want the `gender` and `age_years` columns laterfor bias monitoring.
- Next, we dummy the nominal features.
- Afterwards, we drop columns with 0 variance. (This specific dataset doesn't have any features with 0 variance.)
- Finally, we normalize all the predictors to have a mean of 0 and a standard deviation of 1.

```{r}
gc_recipe <-
    # selecting all columns to predict `label`  
    recipe(label ~ ., data=df_baseline) %>%
    # removing the id, gender, and age_years columns from being predictive variables
    step_rm(id, gender, age_years) %>% 
    # dummying all categorical features
    step_dummy(all_nominal(), -all_outcomes()) %>%
    # dropping columns with 0 variance (constant columns)
    step_zv(all_predictors()) %>%
    # normalizing all columns to have a mean of 0, stdev of 1
    step_normalize(all_predictors())

# check out summary of recipe
summary(gc_recipe)
```

## Model Training
Now that the data is ready, let's instantiate our model and create the workflow. We'll be using a simple logistic regression model for our purposes.

```{r}
# fitting the model, simple logistic regression
logreg <- logistic_reg(penalty=tune(), mixture=tune()) %>% 
    set_engine("glm") %>%
    set_mode("classification")

# creating workflow by combining recipe and model
logreg_wflow <-
    workflow() %>%
    add_model(logreg) %>%
    add_recipe(gc_recipe)
```

Let's go ahead and fit the model. This will be the trained model artifact.
```{r}
# Fitting the model
logreg_fit <- fit(logreg_wflow, df_baseline)
```

Let's make predictions on both the training and testing datasets. We'll add these to copies of the original datasets.

```{r}
# predicting on baseline and sample data
train_preds <- predict(logreg_fit, df_baseline)
test_preds <- predict(logreg_fit, df_sample)

# binding predictions to original dataframes
df_baseline_scored <- bind_cols(train_preds, df_baseline)
df_sample_scored <- bind_cols(test_preds, df_sample)

glimpse(df_baseline_scored)
```

## Evaluating the Model
Now that predictions have been made, let's evaluate our model. We'll go with a standard set of metrics for a binary classifier.

```{r}
# evaluate outcomes
metrics <- metric_set(recall, precision, f_meas, accuracy, kap)
metrics(df_baseline_scored, truth=label, estimate=.pred_class)
metrics(df_sample_scored, truth=label, estimate=.pred_class)
```

Not the best model, but the purpose of this notebook is to get the trained model artifact and use it down the line for a model deployed in Production.

## Saving the trained artifacts


```{r}
# rename columns: label -> label_value, .pred -> score
df_baseline_scored <- rename(df_baseline_scored, label_value=label, score=.pred_class) %>% relocate(label_value, score)
df_sample_scored <- rename(df_sample_scored, label_value=label, score=.pred_class) %>% relocate(label_value, score)

# save "scored" dataframes for later analysis
write_csv(df_baseline_scored, "./data/df_baseline_scored.csv")
write_csv(df_sample_scored, "./data/df_sample_scored.csv")

# persisting the fit model (the trained workflow)
save(logreg_fit, file="trained_model.RData")
```

And that's a wrap! The code below can be used as a stand-alone to test the importing of the trained model artifact and data.

```{r}
# --------------------------------------------------------

# testing loading and predicting with fit model
# run below code without running above code to test

# importing necessary libraries
library(tidymodels)
library(readr)

# importing test data (flatten to prevent nesting)
test_data <-  readr::read_csv("./data/df_sample.csv", show_col_types = FALSE)

# loading fit model
load("trained_model.RData")

# re-assigning model for clarity
model <- logreg_fit

# predicting
predict(model, test_data)
```